{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "$$\n",
    "\\begin{align}\n",
    "X\\!G\\!Boost&=eXtreme+GBDT\\\\\n",
    "&=eXtreme+(Gradient+BDT) \\\\\n",
    "&=eXtreme+Gradient+(Boosting+DecisionTree)\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "$$Boosting \\to BDT \\to GBDT \\to X\\!G\\!Boost$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 提升方法（Boosting）  \n",
    "&emsp;&emsp;提升方法使用加法模型和前向分步算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; &emsp;加法模型\n",
    "$$f\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.1}$$\n",
    "其中，$b\\left(x;\\gamma_m\\right)$为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在给定训练数据及损失函数$L\\left(y,f\\left(x\\right)\\right)$的条件下，学习加法模型$f\\left(x\\right)$成为经验风险极小化即损失函数极小化问题：\n",
    "$$\\min_{\\beta_m,\\gamma_m}\\sum_{i=1}^N L\\left(y_i,\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right)\\right)\\tag{1.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前向分步算法求解这一优化问题的思路：因为学习的是加法模型，可以从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（1.2），则可以简化优化复杂度。具体地，每步只需优化如下损失函数：\n",
    "$$\\min_{\\beta,\\gamma}\\sum_{i=1}^N L\\left(y_i,\\beta b\\left(x_i;\\gamma\\right)\\right)\\tag{1.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法1.1 前向分步算法\n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$；基函数集合$\\{b\\left(x;\\gamma\\right)\\}$；   \n",
    "输出：加法模型$f\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）极小化损失函数\n",
    "$$\\left(\\beta_m,\\gamma_m\\right)=\\mathop{\\arg\\min}_{\\beta,\\gamma} \\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i;\\gamma\\right)\\right) \\tag{1.4}$$  \n",
    "得到参数$\\beta_m$，$\\gamma_m$  \n",
    "（b）更新\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.5}$$  \n",
    "（3）得到加法模型  \n",
    "$$f\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前向分步算法将同时求解从$m=1$到$M$所有参数$\\beta_m,\\gamma_m$的优化问题简化为逐次求解各个$\\beta_m, \\gamma_m$的优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 提升决策树 （BDT，Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以决策树为基函数的提升方法为提升决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树模型可以表示为决策树的加法模型：  \n",
    "$$f_M=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right) \\tag{2.1}$$  \n",
    "其中，$T\\left(x;\\Theta_m\\right)$表示决策树；$\\Theta_m$为决策树的参数；$M$为树的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树采用前向分步算法。首先确定初始提升决策树$f_0\\left(x\\right)=0$，第$m$步的模型是\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) \\tag{2.2}$$\n",
    "其中，$f_{m-1}\\left(x\\right)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$，\n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i,f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right) \\tag{2.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;已知训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots\\left(x_N,y_N\\right)\\}$，$x_i\\in\\mathcal{X}\\subseteq\\mathbb{R}^n$，$\\mathcal{X}$为输入空间，$y_i\\in\\mathcal{Y}\\subseteq\\mathbb{R}$，$\\mathcal{Y}$为输出空间。如果将输入空间$\\mathcal{X}$划分为$J$个互不相交的区域$R_1,R_2,\\dots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么决策树可表示为\n",
    "$$T\\left(x;\\Theta\\right)=\\sum_{j=1}^J c_j I\\left(x\\in R\\right) \\tag{2.4}$$\n",
    "其中，参数$\\Theta=\\{\\left(R_1,c_1\\right),\\left(R_2,c_2\\right),\\dots,\\left(R_J,c_J\\right)\\}$表示决策树的区域划分和各区域上的常量值。$J$是决策树的复杂度即叶子结点个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树使用以下前向分步算法：\n",
    "$$\\begin{align}\n",
    "f_0\\left(x\\right)&=0 \\\\\n",
    "f_m\\left(x\\right)&=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right),\\quad m=1,2,\\dots,M        \\\\\n",
    "f_M\\left(x\\right)&=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)\n",
    "\\end{align}$$  \n",
    "在前向分步算法的第$m$步，给定当前模型$f_{m-1}\\left(x\\right)$，需要求解  \n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i,f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right)$$\n",
    "得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当采用平方误差损失函数时，\n",
    "$$L\\left(y,f\\left(x\\right)\\right)=\\left(y-f\\left(x\\right)\\right)^2$$\n",
    "其损失变为\n",
    "$$\\begin{align}\n",
    "L\\left(y,f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right)\\right) \n",
    "&=\\left[y-f_{m-1}\\left(x\\right)-T\\left(x;\\Theta_m\\right)\\right]^2 \\\\\n",
    "&=\\left[r-T\\left(x;\\Theta_m\\right)\\right]^2\n",
    "\\end{align}$$\n",
    "其中，\n",
    "$$r=y-f_{m-1}\\left(x\\right) \\tag{2.5}$$\n",
    "是当前模型你和数据的残差（residual）。对回归问题的提升决策树，只需要简单地拟合当前模型的残差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法2.1 回归问题的提升决策树算法\n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$；   \n",
    "输出：提升决策树$f_M\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）按照式（2.5）计算残差\n",
    "$$r_{mi}=y_i-f_{m-1}\\left(x_i\\right), \\quad i=1,2,\\dots,N$$  \n",
    "（b)拟合残差$r_{mi}$学习一个回归树，得到$T\\left(x;\\Theta_m\\right)$  \n",
    "（c）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) $  \n",
    "（3）得到回归提升决策树 \n",
    "$$f_M\\left(x\\right)=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;梯度提升算法使用损失函数的负梯度在当前模型的值\n",
    "$$-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)}$$\n",
    "作为回归问题提升决策树算法中残差的近似值，拟合一个回归树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法3.1 梯度提升算法\n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$  \n",
    "输出：梯度提升决策树$\\hat{f}\\left(x\\right)$  \n",
    "（1）初始化\n",
    "$$f_0\\left(x\\right)=\\mathop{\\arg\\min}_c\\sum_{i=1}^N L\\left(y_i,c\\right)$$\n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）对$i=1,2,\\dots,N$，计算\n",
    "$$r_{mi}=-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)}$$  \n",
    "（b)对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},j=1,2,\\dots,J$  \n",
    "（c）对$j=1,2,\\dots,J$，计算\n",
    "$$c_{mj}=\\mathop{\\arg\\min}_c\\sum_{x_i\\in R_{mj}} L\\left(y_i, f_{m-1}\\left(x_i\\right)+c\\right)$$  \n",
    "（d）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)$  \n",
    "（3）得到回归梯度提升决策树 \n",
    "$$\\hat{f}\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 极致梯度提升决策树（XGBoost，eXtreme Gradient Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;训练数据集$\\mathcal{D}=\\{\\left(\\mathbf{x}_i,y_i\\right)\\}$，其中$\\mathbf{x}_i\\in\\mathbb{R}^m,y_i\\in\\mathbb{R},\\left|\\mathcal{D}\\right|=n$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;决策树模型\n",
    "$$f\\left(\\mathbf{x}\\right)=w_{q\\left(\\mathbf{x}\\right)}$$\n",
    "其中，$q:\\mathbb{R}^m\\to T,w\\in\\mathbb{R}^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树模型预测输出\n",
    "$$\\hat{y}_i=\\phi\\left(\\mathbf{x}_i\\right)=\\sum_{k=1}^K f_k\\left(\\mathbf{x}_i\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正则化目标函数\n",
    "$$\\mathcal{L}\\left(\\phi\\right)=\\sum_i l\\left(\\hat{y}_i,y_i\\right)+\\sum_k \\Omega\\left(f_k\\right)$$\n",
    "其中，$\\Omega\\left(f\\right)=\\gamma T+\\frac{1}{2}\\lambda\\|w\\|^2=\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第$t$轮目标函数\n",
    "$$\\mathcal{L}^{\\left(t\\right)}=\\sum_{i=1}^n l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}_i+f_t\\left(\\mathbf{x}_i\\right)\\right)+\\Omega\\left(f_t\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第$t$轮目标函数的二阶泰勒展开\n",
    "$$\\mathcal{L}^{\\left(t\\right)}\\simeq\\sum_{i=1}^n\\left[l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)+g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)$$\n",
    "其中，$g_i=\\partial_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right),h_i=\\partial^2_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第$t$轮目标函数的二阶泰勒展开移除关于\n",
    "$f_t\\left(\\mathbf{x}_i\\right)$常数项\n",
    "$$\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{\\left(t\\right)}&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)  \\\\\n",
    "&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2\n",
    "\\end{align}$$  \n",
    "定义叶结点$j$上的样本的下标集合$I_j=\\{i|q\\left(\\mathbf{x}_i\\right)=j\\}$，则目标函数可表示为按叶结点累加的形式\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}=\\sum_{j=1}^T\\left[\\left(\\sum_{i\\in I_j}g_i\\right)w_j+\\frac{1}{2}\\left(\\sum_{i\\in I_j}h_i+\\lambda\\right)w_j^2\\right]+\\lambda T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$$w_j^*=\\mathop{\\arg\\min}_{w_j}\\tilde{\\mathcal{L}}^{\\left(t\\right)}$$\n",
    "可令$$\\frac{\\partial\\tilde{\\mathcal{L}}^{\\left(t\\right)}}{\\partial w_j}=0$$\n",
    "得到每个叶结点$j$的最优分数为\n",
    "$$w_j^*=-\\frac{\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j} h_i+\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入每个叶结点$j$的最优分数，得到最优化目标函数值\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}\\left(q\\right)=-\\frac{1}{2}\\sum_{j=1}^T \n",
    "\\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda}+\\gamma T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设$I_L$和$I_R$分别为分裂后左右结点的实例集，令$I=I_L\\cup I_R$，则分裂后损失减少量由下式得出\n",
    "$$\\mathcal{L}_{split}=\\frac{1}{2}\\left[\\frac{\\left(\\sum_{i\\in I_L} g_i\\right)^2}{\\sum_{i\\in I_L}h_i+\\lambda}+\\frac{\\left(\\sum_{i\\in I_R} g_i\\right)^2}{\\sum_{i\\in I_R}h_i+\\lambda}-\\frac{\\left(\\sum_{i\\in I} g_i\\right)^2}{\\sum_{i\\in I}h_i+\\lambda}\\right]-\\gamma$$\n",
    "用以评估待分裂结点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法4.1 分裂查找的精确贪婪算法\n",
    "输入：当前结点实例集$I$;特征维度$d$  \n",
    "输出：根据最大分值分裂  \n",
    "（1）$gain\\leftarrow 0$  \n",
    "（2）$G\\leftarrow\\sum_{i\\in I}g_i$，$H\\leftarrow\\sum_{i\\in I}h_i$  \n",
    "（3）for $k=1$ to $m$ do  \n",
    "（3.1）$G_L \\leftarrow 0$，$H_L \\leftarrow 0$  \n",
    "（3.2）for $j$ in sorted($I$, by $\\mathbf{x}_{jk}$) do  \n",
    "（3.2.1）$G_L \\leftarrow G_L+g_j$，$H_L \\leftarrow H_L+h_j$  \n",
    "（3.2.2）$G_R \\leftarrow G-G_L$，$H_R=H-H_L$  \n",
    "（3.2.3）$score \\leftarrow \\max\\left(score,\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G^2}{H+\\lambda}\\right)$  \n",
    "（3.3）end  \n",
    "（4）end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
